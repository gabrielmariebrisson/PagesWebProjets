# app.py
import streamlit as st
import pandas as pd
import numpy as np
import tensorflow as tf
import pickle
import requests
from collections import defaultdict

# --- Fonctions de l'API TMDb ---
@st.cache_data(ttl=3600)
def get_movie_data(title):
    """R√©cup√®re les infos du film depuis OMDb"""
    year = None
    clean_title = title
    if title[-1] == ")" and "(" in title:
        try:
            year = title.split("(")[-1][:-1]
            clean_title = title.rsplit("(", 1)[0].strip()
        except:
            pass

    url = f"http://www.omdbapi.com/?t={clean_title}&apikey=667fd579"
    if year:
        url += f"&y={year}"

    try:
        response = requests.get(url).json()
        if response.get("Response") == "True":
            return {
                "title": response["Title"],
                "year": response["Year"],
                "genre": response.get("Genre", "N/A"),
                "director": response.get("Director", "N/A"),
                "actors": response.get("Actors", "N/A"),
                "plot": response.get("Plot", "N/A"),
                "rating": response.get("imdbRating", "N/A"),
                "votes": response.get("imdbVotes", "0"),
                "poster": response.get("Poster", None),
            }
    except requests.RequestException:
        return None

    return None

# --- Fonctions de chargement ---
@st.cache_resource
def load_model():
    try:
        def l2_norm(x):
            return tf.linalg.l2_normalize(x, axis=1)

        def diff_abs(x):
            return tf.abs(x[0] - x[1])

        def prod_mul(x):
            return x[0] * x[1]

        return tf.keras.models.load_model(
            "./templates/assets/film/best_model.keras",
            custom_objects={
                'l2_norm': l2_norm,
                'diff_abs': diff_abs,
                'prod_mul': prod_mul
            },
            safe_mode=False
        )
    
    except Exception as e:
        st.error(f"Erreur lors du chargement du mod√®le : {e}")
        return None

@st.cache_data
def load_objects():
    try:
        with open('./templates/assets/film/scalerUser.pkl', 'rb') as f: scalerUser = pickle.load(f)
        with open('./templates/assets/film/scalerItem.pkl', 'rb') as f: scalerItem = pickle.load(f)
        with open('./templates/assets/film/scalerTarget.pkl', 'rb') as f: scalerTarget = pickle.load(f)
        with open('./templates/assets/film/movie_dict.pkl', 'rb') as f: movie_dict = pickle.load(f)
        with open('./templates/assets/film/item_vecs_finder.pkl', 'rb') as f: item_vecs_finder = pickle.load(f)
        with open('./templates/assets/film/unique_genres.pkl', 'rb') as f: unique_genres = pickle.load(f)
        return scalerUser, scalerItem, scalerTarget, movie_dict, item_vecs_finder, unique_genres
    except FileNotFoundError:
        st.error("Fichiers .pkl manquants. Assurez-vous d'avoir ex√©cut√© le script de sauvegarde.")
        return (None,) * 6

# --- Fonction de recommandation ---
def generate_recommendations(model, user_ratings, scalers, data):
    scalerUser, scalerItem, scalerTarget = scalers
    movie_dict, item_vecs, unique_genres = data

    if not all([model, scalerUser, scalerItem, scalerTarget, movie_dict, item_vecs is not None, unique_genres]):
        return pd.DataFrame()

    num_ratings = len(user_ratings)
    avg_rating = np.mean(list(user_ratings.values()))
    genre_ratings = defaultdict(list)
    for movie_id, rating in user_ratings.items():
        genres_str = movie_dict[movie_id]['genres']
        if pd.notna(genres_str) and genres_str != "(no genres listed)":
            for genre in genres_str.split('|'):
                genre_ratings[genre].append(rating)

    user_prefs = {f'pref_{g}': np.mean(genre_ratings.get(g, [avg_rating])) for g in unique_genres}
    user_vec = np.array([[num_ratings, avg_rating, 0] + list(user_prefs.values())])
    
    num_items = len(item_vecs)
    user_vecs_repeated = np.tile(user_vec, (num_items, 1))

    suser_vecs = scalerUser.transform(user_vecs_repeated)
    sitem_vecs = scalerItem.transform(item_vecs[:, 1:])

    predictions = model.predict([suser_vecs, sitem_vecs])
    predictions_rescaled = scalerTarget.inverse_transform(predictions)

    recommendations = []
    for i, item_id in enumerate(item_vecs[:, 0]):
        if int(item_id) not in user_ratings:
            recommendations.append({
                'Movie ID': int(item_id),
                'Titre': movie_dict[int(item_id)]['title'],
                'Genres': movie_dict[int(item_id)]['genres'],
                'Note Pr√©dite': predictions_rescaled[i][0]
            })

    reco_df = pd.DataFrame(recommendations)
    return reco_df.sort_values(by='Note Pr√©dite', ascending=False)

# Bouton de redirection
st.markdown(
    """
    <a href="https://gabriel.mariebrisson.fr" target="_blank" style="text-decoration:none;">
    <div style="
    display: inline-block;
    background: linear-gradient(135deg, #6A11CB 0%, #2575FC 100%);
    color: white;
    padding: 12px 25px;
    border-radius: 30px;
    text-align: center;
    font-size: 16px;
    font-weight: 600;
    cursor: pointer;
    box-shadow: 0 4px 15px rgba(37, 117, 252, 0.3);
    transition: all 0.3s ease;
    text-transform: uppercase;
    letter-spacing: 1px;
    border: 2px solid transparent;
    position: relative;
    overflow: hidden;
    ">
    Retour
    <span style="
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background: rgba(255,255,255,0.2);
    transform: scaleX(0);
    transform-origin: right;
    transition: transform 0.3s ease;
    z-index: 1;
    "></span>
    </div>
    </a>
    """,
    unsafe_allow_html=True
)

# --- Interface Streamlit ---
st.set_page_config(layout="wide", page_title="Cin√©-Reco")
st.title("üé¨ Cin√©-Reco : Votre Guide Cin√©ma Personnalis√©")


model = load_model()
scalerUser, scalerItem, scalerTarget, movie_dict, item_vecs_finder, unique_genres = load_objects()


if model and movie_dict:
    # --- Barre lat√©rale pour la notation ---
    st.sidebar.header("üîç Notez des films")
    
    if 'user_ratings' not in st.session_state:
        st.session_state.user_ratings = {}

    # CORRECTION 1: Recherche HORS du formulaire
    movie_list = sorted([info['title'] for info in movie_dict.values()])
    search_term = st.sidebar.text_input("Rechercher un film √† noter :")
    
    if search_term:
        filtered_movie_list = [m for m in movie_list if search_term.lower() in m.lower()]
    else:
        filtered_movie_list = movie_list[:1000]  # Limiter par d√©faut

    # Le formulaire contient uniquement la s√©lection et la note
    with st.sidebar.form("rating_form"):
        if filtered_movie_list:
            selected_movie_title = st.selectbox("Choisissez un film", filtered_movie_list)
        else:
            st.warning("Aucun film trouv√© avec cette recherche")
            selected_movie_title = None
            
        rating = st.slider("Votre note", 1.0, 5.0, 3.0, 0.5)
        submitted = st.form_submit_button("Ajouter la note")

        if submitted and selected_movie_title:
            movie_id = next((mid for mid, info in movie_dict.items() if info['title'] == selected_movie_title), None)
            if movie_id:
                st.session_state.user_ratings[movie_id] = rating
                st.success(f"Note ajout√©e pour : {selected_movie_title}")

    if st.session_state.user_ratings:
        st.sidebar.subheader("Vos notes :")
        for movie_id, rating in st.session_state.user_ratings.items():
            st.sidebar.write(f"- {movie_dict[movie_id]['title']}: **{rating} / 5.0**")
        if st.sidebar.button("üóëÔ∏è Vider les notes"):
            st.session_state.user_ratings = {}
            st.rerun()

    # --- Affichage principal des recommandations ---
    st.header("üåü Vos Recommandations Personnalis√©es")
    if len(st.session_state.user_ratings) >= 3:
        with st.spinner("Nous pr√©parons votre s√©lection personnalis√©e..."):
            recommendations_df = generate_recommendations(
                model, 
                st.session_state.user_ratings,
                (scalerUser, scalerItem, scalerTarget),
                (movie_dict, item_vecs_finder, unique_genres)
            )
        
        # CORRECTION 2: Gestion des NaN dans les genres
        all_genres = set()
        for genres_str in recommendations_df['Genres'].dropna():
            if genres_str and genres_str != "(no genres listed)":
                for genre in str(genres_str).split('|'):
                    if genre.strip():
                        all_genres.add(genre.strip())
        
        all_genres = sorted(list(all_genres))
        selected_genres = st.multiselect("Filtrer par genre :", all_genres)
        
        if selected_genres:
            def has_selected_genre(genres_str):
                if pd.isna(genres_str) or not genres_str:
                    return False
                return any(g in str(genres_str) for g in selected_genres)
            
            filtered_df = recommendations_df[recommendations_df['Genres'].apply(has_selected_genre)]
        else:
            filtered_df = recommendations_df

        # Affichage avec affiches
        st.subheader(f"Top {min(20, len(filtered_df))} des films pour vous :")
        
        cols = st.columns(5)
        for i, (idx, row) in enumerate(filtered_df.head(20).iterrows()):
            col = cols[i % 5]
            with col:
                movie_data = get_movie_data(row['Titre'])
                
                if movie_data and movie_data["poster"] and movie_data["poster"] != "N/A":
                    st.image(movie_data["poster"], caption=f"{row['Note Pr√©dite']:.1f} ‚≠ê")
                else:
                    st.image("./templates/assets/images/no-poster.jpg", caption=f"{row['Note Pr√©dite']:.1f} ‚≠ê")
                
                with st.expander(f"_{row['Titre']}_"):
                    st.write(f"**Genres :** {movie_data['genre'] if movie_data else row['Genres']}")
                    st.write(f"**Note pr√©dite :** {row['Note Pr√©dite']:.2f}")
                    if movie_data:
                        st.write(f"**Acteurs :** {movie_data['actors']}")
                        st.write(f"**R√©sum√© :** {movie_data['plot']}")
                        st.write(f"**Note IMDb :** {movie_data['rating']} ‚≠ê")
                        st.write(f"**Ann√©e :** {movie_data['year']}")

    else:
        st.info("üëã Bienvenue ! Veuillez noter au moins 3 films dans la barre lat√©rale pour d√©bloquer vos recommandations.")
        
    

    # Section Pr√©sentation
    st.header("Pr√©sentation")
    st.markdown(
        """
        Ce projet vise √† recommender des films enfonction des notes par utilisateur par les utilisateurs. Les informations sont partout aujourd'hui, et des algorithme de recommendation choissent pour nous ce que nous avons le plus de chance d'aimer. Nous retrouvons ses algorithmes partout :

        **Applications potentielles :**
        - **Marketing :** Dans la recommendation de produits, les utilisateurs sont plus susceptibles de trouver des produits similaires √† ceux qu'ils ont d√©j√† achet√©s.
        - **Service client :** Dans la recommandation de services, les utilisateurs sont plus susceptibles de trouver des services similaires √† ceux qu'ils ont d√©j√† utilis√©s
        - **Divertissement :** Dans la recommendation de films, les utilisateurs sont plus susceptibles de trouver des films similaires √† ceux qu'ils ont d√©j√† regard√©s.
        - **Pr√©visions de tendances :** Dans la recommandation de tendances, les utilisateurs sont plus susceptibles de trouver des tendances similaires √† ceux qu'ils ont d√©j√† suivis.
        - **Ressources humaines :** Dans la recommandation de personnes, les utilisateurs sont plus susceptibles de trouver des personnes similaires √† ceux qu'ils ont d√©j√† rencontr√©s.
        - **√âducation :** Dans la recommandation de cours, les utilisateurs sont plus susceptibles de trouver des cours similaires √† ceux qu'ils ont d√©j√† suivis.


        Pour cela, nous avons utilis√© le jeu de donn√©es movielensBeliefs afin d'avoir les films notes des films jusqu'au 1er mai 2024, la liste des genres que nous avons en filtre, avec des notes allant de 0.5 a 5 √©toiles, les films vont de 1874 a 2024 pour 20 763 films et 8 493 utilisateurs donc soit un totale de 2 864 752 notes.
        """
    )

    # Section Architecture du Mod√®le
    st.header("Architecture du Mod√®le")
    st.markdown(
        """
        Pour cette algorithme de recommandation, nous avons choisit une architecture de r√©seau de neurones √† deux branches. Une branche traite les donn√©es utilisateur, tandis que l'autre g√®re les donn√©es des films. Chaque branche comprend plusieurs couches denses, avec des fonctions d'activation ReLU et des techniques de r√©gularisation telles que le dropout pour pr√©venir le surajustement. Les sorties des deux branches sont ensuite combin√©es √† l'aide d'une couche Lambda, qui calcule le score de recommandation en fonction des notes des utilisateurs et des notes des films.
        Cette approche permet de capturer les interactions complexes entre les pr√©f√©rences des utilisateurs et les caract√©ristiques des films, am√©liorant ainsi la pr√©cision des recommandations.
        Attention, ce mod√®le ne prend pas en compte les donn√©es textuelles, l'ordre des notes, ni les donn√©es temporelles. Ainsi les performances sont limit√©es mais restent fiables.

        Le mod√®le de pr√©diction des pr√©f√©rences utilisateur‚Äìfilm est compos√© de plusieurs blocs :
        - **Sous-r√©seau utilisateur et sous-r√©seau item :** chacun est constitu√© de couches enti√®rement connect√©es (Dense) avec normalisation (BatchNormalization) et activation GELU, permettant d‚Äôextraire des repr√©sentations denses et non lin√©aires des utilisateurs et des films.
        - **Couches de r√©gularisation :** des couches Dropout (0.3) et une r√©gularisation L2 l√©g√®re sur les poids limitent le surapprentissage tout en pr√©servant la richesse de l‚Äôespace latent.
        - **Normalisation L2 :** les vecteurs issus des deux r√©seaux sont normalis√©s pour contraindre les embeddings sur une hypersph√®re unitaire, ce qui stabilise la comparaison entre utilisateurs et items.
        - **Couche de fusion :** les repr√©sentations sont combin√©es via la diff√©rence absolue et le produit √©l√©ment par √©l√©ment, permettant au mod√®le de capturer √† la fois la dissimilarit√© et la similarit√© bilin√©aire.
        - **Couche de sortie Dense :** une couche sigmo√Øde fournit la probabilit√© de correspondance entre un utilisateur et un film.

        Les principaux hyperparam√®tres incluent la taille des couches (256, 128, 64), le taux de dropout (0.3), la r√©gularisation L2 (1e-6) et l‚Äôactivation GELU, choisie pour sa continuit√© et sa meilleure propagation du gradient par rapport √† ReLU.
            """
    )
    st.image("./templates/assets/images/Architecture.png", caption="Structure du mod√®le de classification des sentiments", use_container_width=True)

    # Section R√©sultats
    st.header("R√©sultats")
    st.markdown(
        """
        Les techniques les plus efficaces pour limiter le surajustement incluent la r√©gularisation par dropout et la r√©duction de la complexit√© du mod√®le. Ces approches ont permis d'obtenir des r√©sultats significatifs, comme en t√©moignent les courbes ci-dessous.

        En observant les courbes d'apprentissage, nous notons une convergence stable avec une pr√©cision atteignant 75 % sur les donn√©es de test (r√©partition 90/10) et 79 % sur les donn√©es d'apprentissage. Cela d√©montre une bonne capacit√© du mod√®le √† g√©n√©raliser sans trop s'adapter aux sp√©cificit√©s des donn√©es d'entra√Ænement.
        """
    )

    col1, col2 = st.columns(2)

    with col1:
        st.image("./templates/assets/images/accurancy.png", caption="Courbes de pr√©cision", use_container_width=True)

    with col2:
        st.image("./templates/assets/images/loss.png", caption="Courbes de perte", use_container_width=True)

    st.markdown(
        """
        **Performances du mod√®le :**
        - Pr√©cision de 75 % sur les donn√©es de test.
        - Pr√©cision de 79 % sur les donn√©es d'apprentissage.
        - Bonne g√©n√©ralisation sans surajustement.

        En plus de la r√©gularisation, des techniques telles que l'augmentation des donn√©es et l'ajustement des hyperparam√®tres ont √©t√© envisag√©es pour am√©liorer davantage les performances du mod√®le. Cela permettrait non seulement d'optimiser la pr√©cision, mais √©galement d'accro√Ætre la robustesse face √† des donn√©es vari√©es.
        """
    )

    # Section Co√ªt et Maintenance
    st.header("Co√ªt de D√©veloppement")
    st.markdown(
        """
        Pour entra√Æner ce mod√®le, nous avons utilis√© Google Colab, o√π l'entra√Ænement √† dur√© 13 minutes. Voici les sp√©cifications mat√©rielles utilis√©es :

        - **Processeur :** Intel Xeon (simple c≈ìur) cadenc√© √† 2,2 GHz.
        - **RAM :** 12,67 GiB.

        Les performances obtenues montrent une pr√©cision de 75 % sur les donn√©es de test et de 79 % sur les donn√©es d'apprentissage. Le poids du mod√®le charg√© est de 52 Mo, et le temps d'ex√©cution pour un test est de seulement 0,21 seconde.

        **Analyse des co√ªts :**
        - Le co√ªt d'entra√Ænement est raisonnable gr√¢ce √† l'utilisation de Google Colab, qui offre des ressources GPU gratuites pour des projets de petite √† moyenne envergure.
        - Les co√ªts de maintenance incluront principalement les mises √† jour des donn√©es et l'optimisation des hyperparam√®tres.

        **Perspectives d'am√©lioration :**
        - Tester des architectures plus complexes comme les r√©seaux de neurones r√©currents (RNN) ou les Transformers.
        - Utiliser des techniques d'augmentation de donn√©es pour enrichir l'ensemble d'apprentissage.
        - Impl√©menter une validation crois√©e pour mieux √©valuer la robustesse du mod√®le.
        """
    )

else:
    st.error("L'application n'a pas pu d√©marrer. V√©rifiez les fichiers du mod√®le et des donn√©es.")